{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "##                                                                                          ##\n",
    "##                      Python Code to Test Classifier for USPTO Data                        ##\n",
    "##                                                                                          ##\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from nltk.stem import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Import Preprocessing #\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "\n",
    "# Import Cross Val Libraries #\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Import Classifiers #\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LassoCV, SGDClassifier, LinearRegression, LogisticRegression, RidgeCV, RidgeClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Import spacy for STOP WORDS AND WORD2VEC\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "\n",
    "#Immport Keras for Neural Networks\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#Import Word2vec library en_core_web_md, NLTK and lemamtizers\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "nlp = en_core_web_md.load()\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# To silence the warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set default number of non-AI training data points\n",
    "NUM_OF_NON_AI_PATENTS_FOR_TRAINING = 1600\n",
    "\n",
    "# Set how many folds for cross validation\n",
    "NUM_OF_SPLITS = 10\n",
    "\n",
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_AI</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       abstract\n",
       "is_AI          \n",
       "0          2062\n",
       "1          1050"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the file\n",
    "\n",
    "TrainingData= pd.read_csv(\"TData_Export_USPTO.csv\")\n",
    "TrainingData.groupby([\"is_AI\"]).agg({\"abstract\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks for key word presence\n",
    "keywords1 = ['Neural Network', 'Neural Networks', 'Artificial Intelligence', 'Machine Learning', 'Reinforcement Learning',\n",
    "            'Machine Learning', 'Pattern Recognition', 'Bayes', 'Computer Vision', 'Language Processing',\n",
    "            'Natural Language', 'Data Mining']\n",
    "\n",
    "keywords2 = ['Neural Network', 'Artificial Intelligence', 'Machine Learning', 'Reinforcement Learning','Pattern Recognition', 'Bayes',\n",
    "        'Computer Vision', 'Language Processing','Natural Language', 'Data Mining', 'image grammar', 'physical symbol system', 'symbolic error analysis',\n",
    "        'robot', 'pattern recognition', 'image matching', 'machine intelligence',\n",
    "        'logic theorist', 'symbolic reasoning', 'symbolic error analysis', 'supervised learning',\n",
    "        'pattern analysis', 'deep learning', 'collaborative system', 'symbol processing',\n",
    "        'crowdsourcing', 'human computation', 'sensor network', 'neuromorphic computing',\n",
    "        'decision making', 'sensor data fusion', 'layered control systems',\n",
    "        'image processing', 'convolution network', 'recommendation system', 'speech recognition']\n",
    "\n",
    "keywords = \"|\".join(keywords1 + keywords2)\n",
    "#keywords = 'Neural Network|Artificial Intelligence|Machine Learning|Reinforcement Learning|Machine Learning|Pattern Recognition|Bayes|Computer Vision|Language Processing|Natural Language|Data Mining'\n",
    "keywords = keywords.lower()\n",
    "\n",
    "TrainingData[\"AI_Keyword\"] = np.where(TrainingData[\"abstract\"].str.lower().str.contains(keywords),1,0)\n",
    "\n",
    "TrainingData[\"AI_Keyword\"].mean(), TrainingData[\"is_AI\"].mean()\n",
    "\n",
    "TrainingData['app_number'] = np.where(TrainingData['app_number'].notnull(), TrainingData['app_number'], TrainingData['id'] )\n",
    "ids =  TrainingData[\"app_number\"].values.tolist()\n",
    "\n",
    "processed_content_list = TrainingData['abstract'].values.tolist()\n",
    "labels = TrainingData[\"is_AI\"]\n",
    "keyword_labels = TrainingData[\"AI_Keyword\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.6 s, sys: 91 ms, total: 5.69 s\n",
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizing the data\n",
    "\n",
    "%%time\n",
    "\n",
    "def tokenize_lemmatize(x):\n",
    "    \n",
    "    s = ''\n",
    "    for i in nltk.word_tokenize(x):\n",
    "        i = i.lower()\n",
    "        i = lemmatizer.lemmatize(i)\n",
    "        s += i+ ' '\n",
    "\n",
    "    return s.strip()\n",
    "            \n",
    "TrainingData['abstract'] = TrainingData['abstract'].apply(lambda x: tokenize_lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing keras libraries for fitting an LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting max features and limiting length of embeddings to maxlen\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(TrainingData[\"abstract\"])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(TrainingData[\"abstract\"])\n",
    "\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = keras.utils.to_categorical(labels)\n",
    "\n",
    "#Building LSTM \n",
    "\n",
    "def create_network():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 200))\n",
    "    model.add(Bidirectional(LSTM(20, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(15, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"sigmoid\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "neural_network = KerasClassifier(build_fn=create_network, \n",
    "                                 epochs=10, \n",
    "                                 batch_size=256, \n",
    "                                 verbose=0)\n",
    "\n",
    "#model.summary()\n",
    "#model.fit(X_t,y, batch_size=256, epochs= 10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSTM with 10-fold cross validation and 6000 feature BOW model is: 0.7850873957085034\n"
     ]
    }
   ],
   "source": [
    "#LSTM with cross validation\n",
    "scores = cross_val_score(neural_network, X_t, y, cv=10)\n",
    "print(\"Accuracy of LSTM with 10-fold cross validation and 6000 feature BOW model is:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT Implementation\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 220\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 32\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3112 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 24/3112 [00:00<00:12, 238.21it/s]\u001b[A\n",
      "  2%|▏         | 54/3112 [00:00<00:12, 252.27it/s]\u001b[A\n",
      "  3%|▎         | 80/3112 [00:00<00:12, 252.43it/s]\u001b[A\n",
      "  3%|▎         | 108/3112 [00:00<00:11, 258.31it/s]\u001b[A\n",
      "  4%|▍         | 137/3112 [00:00<00:11, 267.03it/s]\u001b[A\n",
      "  5%|▌         | 160/3112 [00:00<00:13, 227.01it/s]\u001b[A\n",
      "  6%|▌         | 182/3112 [00:00<00:14, 203.91it/s]\u001b[A\n",
      "  7%|▋         | 209/3112 [00:00<00:13, 218.20it/s]\u001b[A\n",
      "  8%|▊         | 237/3112 [00:00<00:12, 232.83it/s]\u001b[A\n",
      "  8%|▊         | 261/3112 [00:01<00:12, 231.99it/s]\u001b[A\n",
      "  9%|▉         | 286/3112 [00:01<00:12, 235.12it/s]\u001b[A\n",
      " 10%|▉         | 310/3112 [00:01<00:12, 228.16it/s]\u001b[A\n",
      " 11%|█         | 338/3112 [00:01<00:11, 241.01it/s]\u001b[A\n",
      " 12%|█▏        | 366/3112 [00:01<00:10, 249.79it/s]\u001b[A\n",
      " 13%|█▎        | 397/3112 [00:01<00:10, 264.50it/s]\u001b[A\n",
      " 14%|█▎        | 424/3112 [00:01<00:10, 256.11it/s]\u001b[A\n",
      " 14%|█▍        | 450/3112 [00:01<00:10, 253.89it/s]\u001b[A\n",
      " 15%|█▌        | 478/3112 [00:01<00:10, 259.47it/s]\u001b[A\n",
      " 16%|█▌        | 505/3112 [00:02<00:09, 262.11it/s]\u001b[A\n",
      " 17%|█▋        | 532/3112 [00:02<00:10, 251.54it/s]\u001b[A\n",
      " 18%|█▊        | 558/3112 [00:02<00:10, 248.86it/s]\u001b[A\n",
      " 19%|█▉        | 584/3112 [00:02<00:10, 249.78it/s]\u001b[A\n",
      " 20%|█▉        | 610/3112 [00:02<00:10, 246.60it/s]\u001b[A\n",
      " 20%|██        | 635/3112 [00:02<00:10, 245.12it/s]\u001b[A\n",
      " 21%|██▏       | 667/3112 [00:02<00:09, 261.50it/s]\u001b[A\n",
      " 22%|██▏       | 697/3112 [00:02<00:08, 271.54it/s]\u001b[A\n",
      " 23%|██▎       | 725/3112 [00:02<00:08, 272.62it/s]\u001b[A\n",
      " 24%|██▍       | 753/3112 [00:02<00:08, 274.55it/s]\u001b[A\n",
      " 25%|██▌       | 781/3112 [00:03<00:08, 268.27it/s]\u001b[A\n",
      " 26%|██▌       | 812/3112 [00:03<00:08, 277.58it/s]\u001b[A\n",
      " 27%|██▋       | 840/3112 [00:03<00:08, 272.82it/s]\u001b[A\n",
      " 28%|██▊       | 869/3112 [00:03<00:08, 276.61it/s]\u001b[A\n",
      " 29%|██▉       | 897/3112 [00:03<00:08, 276.17it/s]\u001b[A\n",
      " 30%|██▉       | 925/3112 [00:03<00:08, 264.95it/s]\u001b[A\n",
      " 31%|███       | 952/3112 [00:03<00:08, 262.76it/s]\u001b[A\n",
      " 32%|███▏      | 982/3112 [00:03<00:07, 272.84it/s]\u001b[A\n",
      " 33%|███▎      | 1012/3112 [00:03<00:07, 279.57it/s]\u001b[A\n",
      " 33%|███▎      | 1042/3112 [00:04<00:07, 280.76it/s]\u001b[A\n",
      " 34%|███▍      | 1071/3112 [00:04<00:07, 277.64it/s]\u001b[A\n",
      " 35%|███▌      | 1099/3112 [00:04<00:07, 271.78it/s]\u001b[A\n",
      " 36%|███▋      | 1129/3112 [00:04<00:07, 278.56it/s]\u001b[A\n",
      " 37%|███▋      | 1157/3112 [00:04<00:07, 265.34it/s]\u001b[A\n",
      " 38%|███▊      | 1191/3112 [00:04<00:06, 283.31it/s]\u001b[A\n",
      " 39%|███▉      | 1220/3112 [00:04<00:06, 274.15it/s]\u001b[A\n",
      " 40%|████      | 1248/3112 [00:04<00:06, 270.86it/s]\u001b[A\n",
      " 41%|████      | 1276/3112 [00:04<00:06, 272.73it/s]\u001b[A\n",
      " 42%|████▏     | 1304/3112 [00:04<00:06, 266.93it/s]\u001b[A\n",
      " 43%|████▎     | 1331/3112 [00:05<00:06, 263.04it/s]\u001b[A\n",
      " 44%|████▎     | 1358/3112 [00:05<00:06, 262.58it/s]\u001b[A\n",
      " 45%|████▍     | 1385/3112 [00:05<00:06, 260.45it/s]\u001b[A\n",
      " 46%|████▌     | 1416/3112 [00:05<00:06, 273.36it/s]\u001b[A\n",
      " 46%|████▋     | 1445/3112 [00:05<00:06, 277.66it/s]\u001b[A\n",
      " 47%|████▋     | 1475/3112 [00:05<00:05, 283.80it/s]\u001b[A\n",
      " 48%|████▊     | 1504/3112 [00:05<00:06, 262.46it/s]\u001b[A\n",
      " 49%|████▉     | 1531/3112 [00:05<00:05, 264.42it/s]\u001b[A\n",
      " 50%|█████     | 1558/3112 [00:05<00:05, 261.34it/s]\u001b[A\n",
      " 51%|█████     | 1585/3112 [00:06<00:05, 259.67it/s]\u001b[A\n",
      " 52%|█████▏    | 1612/3112 [00:06<00:05, 257.07it/s]\u001b[A\n",
      " 53%|█████▎    | 1641/3112 [00:06<00:05, 265.16it/s]\u001b[A\n",
      " 54%|█████▎    | 1668/3112 [00:06<00:05, 257.15it/s]\u001b[A\n",
      " 54%|█████▍    | 1694/3112 [00:06<00:06, 232.21it/s]\u001b[A\n",
      " 55%|█████▌    | 1722/3112 [00:06<00:05, 244.17it/s]\u001b[A\n",
      " 56%|█████▌    | 1749/3112 [00:06<00:05, 250.19it/s]\u001b[A\n",
      " 57%|█████▋    | 1775/3112 [00:06<00:05, 250.20it/s]\u001b[A\n",
      " 58%|█████▊    | 1802/3112 [00:06<00:05, 255.02it/s]\u001b[A\n",
      " 59%|█████▊    | 1828/3112 [00:07<00:05, 253.91it/s]\u001b[A\n",
      " 60%|█████▉    | 1864/3112 [00:07<00:04, 277.57it/s]\u001b[A\n",
      " 61%|██████    | 1893/3112 [00:07<00:04, 259.20it/s]\u001b[A\n",
      " 62%|██████▏   | 1920/3112 [00:07<00:04, 256.49it/s]\u001b[A\n",
      " 63%|██████▎   | 1957/3112 [00:07<00:04, 281.03it/s]\u001b[A\n",
      " 64%|██████▍   | 1987/3112 [00:07<00:03, 282.23it/s]\u001b[A\n",
      " 65%|██████▍   | 2016/3112 [00:07<00:03, 274.81it/s]\u001b[A\n",
      " 66%|██████▌   | 2045/3112 [00:07<00:03, 270.75it/s]\u001b[A\n",
      " 67%|██████▋   | 2073/3112 [00:07<00:03, 265.94it/s]\u001b[A\n",
      " 67%|██████▋   | 2100/3112 [00:08<00:04, 251.58it/s]\u001b[A\n",
      " 68%|██████▊   | 2126/3112 [00:08<00:03, 247.78it/s]\u001b[A\n",
      " 69%|██████▉   | 2155/3112 [00:08<00:03, 258.83it/s]\u001b[A\n",
      " 70%|███████   | 2182/3112 [00:08<00:03, 255.77it/s]\u001b[A\n",
      " 71%|███████   | 2208/3112 [00:08<00:03, 256.29it/s]\u001b[A\n",
      " 72%|███████▏  | 2234/3112 [00:08<00:03, 247.29it/s]\u001b[A\n",
      " 73%|███████▎  | 2259/3112 [00:08<00:03, 243.91it/s]\u001b[A\n",
      " 74%|███████▎  | 2290/3112 [00:08<00:03, 259.34it/s]\u001b[A\n",
      " 74%|███████▍  | 2317/3112 [00:08<00:03, 249.67it/s]\u001b[A\n",
      " 75%|███████▌  | 2343/3112 [00:08<00:03, 249.53it/s]\u001b[A\n",
      " 76%|███████▌  | 2369/3112 [00:09<00:03, 239.79it/s]\u001b[A\n",
      " 77%|███████▋  | 2394/3112 [00:09<00:03, 239.07it/s]\u001b[A\n",
      " 78%|███████▊  | 2419/3112 [00:09<00:02, 241.41it/s]\u001b[A\n",
      " 79%|███████▊  | 2444/3112 [00:09<00:02, 239.65it/s]\u001b[A\n",
      " 79%|███████▉  | 2469/3112 [00:09<00:02, 227.82it/s]\u001b[A\n",
      " 80%|████████  | 2494/3112 [00:09<00:02, 233.72it/s]\u001b[A\n",
      " 81%|████████  | 2518/3112 [00:09<00:02, 231.83it/s]\u001b[A\n",
      " 82%|████████▏ | 2542/3112 [00:09<00:02, 227.55it/s]\u001b[A\n",
      " 82%|████████▏ | 2567/3112 [00:09<00:02, 231.77it/s]\u001b[A\n",
      " 83%|████████▎ | 2594/3112 [00:10<00:02, 241.79it/s]\u001b[A\n",
      " 84%|████████▍ | 2620/3112 [00:10<00:01, 246.57it/s]\u001b[A\n",
      " 85%|████████▌ | 2649/3112 [00:10<00:01, 257.01it/s]\u001b[A\n",
      " 86%|████████▌ | 2675/3112 [00:10<00:01, 253.78it/s]\u001b[A\n",
      " 87%|████████▋ | 2701/3112 [00:10<00:01, 248.01it/s]\u001b[A\n",
      " 88%|████████▊ | 2730/3112 [00:10<00:01, 257.78it/s]\u001b[A\n",
      " 89%|████████▊ | 2756/3112 [00:10<00:01, 255.28it/s]\u001b[A\n",
      " 89%|████████▉ | 2782/3112 [00:10<00:01, 255.13it/s]\u001b[A\n",
      " 90%|█████████ | 2808/3112 [00:10<00:01, 255.72it/s]\u001b[A\n",
      " 91%|█████████ | 2834/3112 [00:10<00:01, 254.89it/s]\u001b[A\n",
      " 92%|█████████▏| 2860/3112 [00:11<00:01, 249.56it/s]\u001b[A\n",
      " 93%|█████████▎| 2886/3112 [00:11<00:00, 251.24it/s]\u001b[A\n",
      " 94%|█████████▍| 2920/3112 [00:11<00:00, 270.46it/s]\u001b[A\n",
      " 95%|█████████▍| 2949/3112 [00:11<00:00, 275.10it/s]\u001b[A\n",
      " 96%|█████████▌| 2979/3112 [00:11<00:00, 280.80it/s]\u001b[A\n",
      " 97%|█████████▋| 3012/3112 [00:11<00:00, 292.91it/s]\u001b[A\n",
      " 98%|█████████▊| 3044/3112 [00:11<00:00, 300.50it/s]\u001b[A\n",
      " 99%|█████████▉| 3075/3112 [00:11<00:00, 301.58it/s]\u001b[A\n",
      "100%|█████████▉| 3106/3112 [00:11<00:00, 289.50it/s]\u001b[A\n",
      "100%|██████████| 3112/3112 [00:11<00:00, 260.46it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "X_bert = convert_lines(TrainingData[\"abstract\"], MAX_SEQUENCE_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30135"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Available max features\n",
    "data.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 35000\n",
    "maxlen = 130\n",
    "X_bert = pad_sequences(X_bert, maxlen=maxlen)\n",
    "y = keras.utils.to_categorical(labels)\n",
    " \n",
    "def create_network():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 200))\n",
    "    model.add(Bidirectional(LSTM(20, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(15, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"sigmoid\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "neural_network = KerasClassifier(build_fn=create_network, \n",
    "                                 epochs=10, \n",
    "                                 batch_size=256, \n",
    "                                 verbose=0)\n",
    "\n",
    "#model.summary()\n",
    "#model.fit(X_t,y, batch_size=256, epochs= 10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSTM with 10-fold cross validation and 35k feature BERT model is: 0.7516788296913194\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(neural_network, X_bert, y, cv=10)\n",
    "print(\"Accuracy of LSTM with 10-fold cross validation and 35k feature BERT model is:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
